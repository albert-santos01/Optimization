{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"a948U3waVysn"},"source":["# Group Members:\n","\n","*   Name 1 Albert u186359\n","*   Name 2 Elsa u187334   \n","*   Name 3 Héctor u186408"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txGheWTDcZj9"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ex0Rz-TYciF5"},"outputs":[],"source":["# %cd \"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4TiRpb5crr_"},"outputs":[],"source":["# !pwd"]},{"cell_type":"markdown","metadata":{"id":"Cpcp1FQEbvOT"},"source":["# Import required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEFf8frqbvOg"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"8Dp7AHMkbvOi"},"source":["# 1. Linear regression\n","\n","## 1.1 Computing the minimum via the normal equations"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{"id":"GBSQgR-dbvOi"},"source":["#### 1 Complete the function polyfit_inv_normal_eq for computing w&ast;\n","\n","  $$w^* = (\\Phi^{T}\\Phi)^{-1}\\Phi^{T}y$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KLfz4VWxbvOj"},"source":["**1) Calculate Phi**\n","\n","$$\\Phi = \\begin{bmatrix} \n","1 & x_{1} & x_{1}^{2} & x_{1}^{3}\\\\\n","1 & x_{2} & x_{2}^{2} & x_{2}^{3}\\\\\n","\\vdots & \\vdots & \\vdots & \\vdots\\\\\n","1 & x_{m} & x_{m}^{2} & x_{m}^{3}\n","\\end{bmatrix}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBlLaZkHbvOk"},"outputs":[],"source":["#TOY EXAMPLE\n","\n","# Define x --> (m, 1) \n","# creates a matrix x that has the values 2, 3, 4 arranged vertically (T)\n","\n","x = np.matrix([range(2,5)]).T\n","\n","# Define the number of coefficients to calculate (components of w --> w = (w0, w1, ..., wn))\n","num_coefficients = 4\n","\n","# Calculate phi (do the design matrix)\n","# Create a matrix inited with 0s which will be mxn and have the m of x and n as the number of coefficients \n","# Will follow the structure as specified\n","phi = np.matrix(np.zeros((x.shape[0], num_coefficients))) \n","\n","for i in range(num_coefficients): \n","    phi[:,i] = np.power(x, i)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lfh46vR3poq"},"outputs":[],"source":["print(\"Independent variable: \\n\",x,\"\\n\\nDesign matrix:\\n\",phi)"]},{"cell_type":"markdown","metadata":{"id":"MW6WabPgbvOm"},"source":["**2) Calculate w&ast;**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FlNBUMrbvOn"},"outputs":[],"source":["# Define y --> (m, 1) \n","y = np.matrix([range(2,5)]).T\n","\n","# First of all if phi^t * phi is invertible then we can calculate w* = (phi^T * phi)^-1 * phi^T * y \n","# inv worked then it is invertible\n","w_ast = np.linalg.inv(phi.T * phi) * phi.T * y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["w_ast"]},{"cell_type":"markdown","metadata":{"id":"SPFjWNAGbvOp"},"source":["**Function definition**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6BjA6IdbvOp"},"outputs":[],"source":["def polyfit_inv_normal_eq(x: np.matrix,\n","                          y: np.matrix,\n","                          n: int):\n","    \"\"\"\n","    Fits a polynomial of degree n to a sets of samples x and y.\n","    The polynomial minimizes the sum of squared errors (least squares)\n","    solving the normal equations.\n","\n","    :param x: m x 1, points in the x axis where the function is known\n","    :param y: m x 1, known values of the function at positions in x\n","    :param n: number of coefficients (degree of the polynomial + 1)\n","\n","    :return w: n x 1, vector of polynomial coefficients\n","    :return phi: m x n, data matrix\n","    \"\"\"\n","    # TODO: create phi\n","    # Create a matrix inited with 0s which will be mxn and have the m of x and n as the number of coefficients\n","    m=x.shape[0]\n","    phi = np.matrix(np.zeros((m, n)))\n","\n","    # Calculate phi\n","    # Will follow the structure as specified\n","    for i in range(n): \n","        phi[:,i] = np.power(x, i)\n","    \n","    # TODO: create w*\n","    # Calculate w* = (phi^T * phi)^-1 * phi^T * y by using the normal equations\n","    w_ast = np.linalg.inv(phi.T*phi)*phi.T*y\n","    \n","    return phi, w_ast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8l2ApyjkbvOq"},"outputs":[],"source":["polyfit_inv_normal_eq(x, y, num_coefficients)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Note: If the number of equations 'm' increases the unknown parameters w will converge to (0,1,0,0)^T"]},{"cell_type":"markdown","metadata":{"id":"DnNa1hVybvOr"},"source":["#### 2. Complete the function polyfit_main, to verify that the w&ast; returned by polyfit_inv_normal_eq satisfies that the residue r = y − &phi;w&ast; is orthogonal to Im&phi;, i.e. r ⊥ Im&phi;. Hint: Im&phi; is the space generated by the columns of &phi;."]},{"cell_type":"markdown","metadata":{"id":"ISwwHsxkbvOr"},"source":["**Generate random data**\n","\n","We will generate an artificial data set. We do it by generating random\n","samples from the following polynomial:\n"," \n","p(x) = w<sub>0</sub> + w<sub>1</sub>\\*x + w<sub>2</sub>\\*x<sup>2</sup> + w<sub>3</sub>\\*x<sup>3</sup> \n"," \n","We generate some x<sub>i</sub>, with their corresponding y<sub>i</sub>, and then add random noise to the y<sub>i</sub> to simulate errors in the measurement. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YkAYUeArKjyE"},"outputs":[],"source":["#Bigger example\n","m=1000\n","# Set seed value. Use same whenever calling a random number generator\n","seed_value = 2 \n","np.random.seed(seed_value)\n","x = np.matrix(np.random.uniform(-13,13,(m,1)))\n","y = np.power(x,range(0,num_coefficients))@np.random.uniform(size=(num_coefficients,1)) + np.random.uniform(-1,1,size=(m,1))"]},{"cell_type":"markdown","metadata":{"id":"KdoRw2iKbvOt"},"source":["Build up a graphic that shows p(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHC_Lg0DbvOt"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShxAVAm8bvOt"},"outputs":[],"source":["# Plot the data\n","plt.plot(x, y, '.')"]},{"cell_type":"markdown","metadata":{"id":"z_ivUiffbvOu"},"source":["**Calculate the residue**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Call the function did before\n","phi, w_ast = polyfit_inv_normal_eq(x, y, num_coefficients)\n","# Calculate the residue\n","r = y - phi*w_ast"]},{"cell_type":"markdown","metadata":{"id":"YRmF677ibvOv"},"source":["**Demostrate r ⊥ Im&phi;**\n","\n","To demonstrate that r ⊥ Im&phi; we need to check that the scalar product of both matrixes is 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2P7mOGXbvOv"},"outputs":[],"source":["# z vector belonging to the image\n","z=np.array([[1,1,1,1]]).T\n","imPhi= phi*z\n","print(imPhi.shape)\n","\n","# do the scalar product which must be 0 or close to be orthogonal\n","scalar_product = imPhi.T *r\n","\n","print(\"Scalar product matrix: \\n\\n{}\\n\".format(scalar_product))\n","# print(\"Scalar product solution: {}\".format(scalar_value))"]},{"cell_type":"markdown","metadata":{"id":"gaar012rbvOw"},"source":["**Function definition**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZ3eg5jCbvOw"},"outputs":[],"source":["def polyfit_main(n_data_samples: int = 100,\n","                 polynomial_size: int = 4,\n","                 lower_data_value: int = -10,\n","                 higher_data_value: int = 10):\n","    \"\"\"\n","    Fits a polynomial of degree n to a set of samples.\n","\n","    :param n_data_samples: number of samples to be created\n","    :param polynomial_size: polynomial size (number of elements in array)\n","    :param lower_data_value: lowest value in data\n","    :param higher_data_value: highest value in data\n","\n","    :return:\n","    \"\"\"\n","    # TODO: create x\n","    # linspace creates a linearly spaced vector with n_data_samples elements between lower_data_value and higher_data_value \n","    x = np.matrix(np.linspace(lower_data_value,higher_data_value, n_data_samples)).T\n","\n","    # TODO: create y to fit p(x) = w0 + w1 * x + w2 * x^2 + w3 * x ^ 3 + ... + wn * x ^ n --> n = polynomial_size\n","    # y will be a matrix of 0s with n_data_samples rows and 1 column\n","    y = np.matrix(np.zeros((n_data_samples, 1)))\n","    for i in range(polynomial_size + 1):\n","        y += np.power(x, i)\n","\n","    # Plot the graphic\n","    plt.figure()\n","    plt.plot(x, y, '.')\n","    plt.show()\n","\n","    # TODO: Calculate w*\n","    # Calculate w* = (phi^T * phi)^-1 * phi^T * y\n","    phi, w_ast = polyfit_inv_normal_eq(x, y, polynomial_size)\n","\n","    # TODO: Calculate the residue and demostrate it is orthogonal to Phi\n","    # Calculate the residue\n","    r = y - phi*w_ast\n","    # Calculate the scalar product\n","    scalar_product = phi.T * r\n","    scalar_value = np.average(scalar_product)\n","    print(\"Scalar product matrix: \\n\\n{}\\n\".format(scalar_product))\n","    print(\"Scalar product solution: {}\".format(scalar_value))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y0SfeJjMbvOx"},"outputs":[],"source":["polyfit_main()"]},{"cell_type":"markdown","metadata":{"id":"S5eOgdLfbvOy"},"source":["## 1.2 Minimization with the SVD"]},{"cell_type":"markdown","metadata":{"id":"dwqxmGSSbvOy"},"source":["#### The pseudo-inverse"]},{"cell_type":"markdown","metadata":{"id":"EKeIk8QzbvOz"},"source":["##### 1) Generate a 5 × 3 random matrix, A. Compute its SVD A = USV<sup>T</sup> using the command svd. Verify that A = USV<sup>T</sup>."]},{"cell_type":"markdown","metadata":{"id":"eQNyvYQ8bvOz"},"source":["**Generate A**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j06_JknIbvOz"},"outputs":[],"source":["# Generate a 5 × 3 random matrix\n","# rand(5,3) means 5 rows and 3 columns\n","A = np.matrix(np.random.rand(5,3))\n","print(A)"]},{"cell_type":"markdown","metadata":{"id":"uBuM4Rl4bvO0"},"source":["**Compute its SVD**\n","\n","Numpy provides a function to compute the SVD of a given matrix. It is accesible from its *linalg* object, and it will return U, S and V<sup>T</sup>. Moreover, S is not a rectangular diagonal matrix as expected, so in order to perform real operations with it you must use *diag(S)*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3eiXRh_bvO0"},"outputs":[],"source":["# Calculate the SVD\n","U, S, Vt = np.linalg.svd(A)\n","# Convert S to a diagonal matrix m x n\n","S = np.diag(S)\n","Sz= np.zeros((5,3))\n","Sz[:3,:3]=S\n","S=Sz\n","\n","\n","print(\"U: \\n\\n{}\\n\\nS: \\n\\n{}\\n\\nVt: \\n\\n{}\".format(U, S, Vt))"]},{"cell_type":"markdown","metadata":{"id":"9mkiYJEIbvO1"},"source":["**Verify that A = USV<sup>T</sup>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMZcaNMYbvO2"},"outputs":[],"source":["# Calculate A_recovered = U * S * Vt\n","A_recovered = U * S * Vt\n","\n","# Check if calculated A is equal to, or really close to, the original A\n","print(\"Is verified that A = U * S * transpose(V): {}\".format(np.allclose(A, A_recovered)))"]},{"cell_type":"markdown","metadata":{"id":"OuCCIOggbvO3"},"source":["**Verify that U and V are orthogonal matrixes**\n","\n","To verify that we will use the following consideration: If U and V are orthogonal, then (U * V)<sup>-1</sup> = (U * V)<sup>T</sup> "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVin660SbvO3"},"outputs":[],"source":["# Verify that U and V are orthogonal matrixes\n","\n","U3=U[:,:3] # U3 is the first 3 columns of U in order to do the proof\n","print((U).shape, U3.shape)\n","# multiply matrices U*V\n","Q = np.matmul(U3,Vt.T)\n","\n","B = Q.I #computing inverse of (U * V) \n","C = Q.T #computing transpose (U * V)\n","\n","print(\"Q\\n\\n{}\\n\\nB\\n\\n{}\\n\\nC\\n\\n{}\\n\\nIs U ⊥ V? {}\".format(Q, B, C, np.allclose(B, C)))"]},{"cell_type":"markdown","metadata":{"id":"Tu9nAs8abvO4"},"source":["##### 2) Compute the pseudo-inverse of S, S†. Use it to compute the pseudo-inverse of A, A†. Verify that A†A is the identity matrix of size 3. What happens with AA†?"]},{"cell_type":"markdown","metadata":{"id":"Y_GS-8JdbvO4"},"source":["**Compute the pseudo-inverse of S**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQwyrnt4bvO5"},"outputs":[],"source":["# Compute the pseudo-inverse of S using the function pinv (S is a diagonal matrix)\n","S_pinv = np.linalg.pinv(S)\n","print(S_pinv)"]},{"cell_type":"markdown","metadata":{"id":"lsomz0x4bvO5"},"source":["**Compute the pseudo-inverse of A by using the pseudo-inverse of S**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0uxaoZ4XbvO5"},"outputs":[],"source":["# By formula in the labs instructions\n","A_pinv = Vt.T * S_pinv * U.T\n","print(A_pinv)"]},{"cell_type":"markdown","metadata":{"id":"vyad16ktbvO6"},"source":["**Verify that A†A is the identity matrix of size 3**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKUB2s2ObvO6"},"outputs":[],"source":["# Verify that A†A is the identity matrix of size 3\n","\n","I_ = A_pinv * A\n","print(\"A†A:\\n\\n{}\\n\".format(I_))\n","print(\"A†A is the identity matrix of size 3?: {}\".format(np.allclose(I_, np.identity(n = len(I_)))))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# #TODO: What will happen with AA†?\n","#AA†\n","Result= A*A_pinv\n","print(\"AA†:\\n\\n{}\\n\".format(Result))\n","print(\"A†A is the identity matrix of size 3?: {}\".format(np.allclose(Result, np.identity(n = len(Result)))))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**What will happen with AA†?**\n","What will happen is that the AA† will be a matrix 5x5, not equal to the identity matrix.\n"," This happens because of the properties of the original matrix A on which the own properties of A†A and AA† depend. In this case A is a rectangular matrix (mxn, 5x3) with more rows than columns (m > n), then A†A will be an n x n identity matrix(3x3), but AA† will not be an identity matrix as expected.\n","If we had the opposite case A being more columns than rows (n > m) (3x5) then AA† we would expect for it to be the identity matrix and A†A not. \n","Some other interesting properties about AA† are that the matrix is symmetric and semidefinite positive."]},{"cell_type":"markdown","metadata":{"id":"NCgJoqfCbvO7"},"source":["#### Solving least squares with the pseudo-inverse"]},{"cell_type":"markdown","metadata":{"id":"aghP0dtMbvO7"},"source":["#### 5. Complete the function polyfit_svd_normal_eq for computing w\\*\n","\n","$$w^{*} = \\Phi^{\\dagger}y = VS^{\\dagger}U^{T}y$$"]},{"cell_type":"markdown","metadata":{"id":"SIkKrWlSbvO8"},"source":["**1) Calculate Phi**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4fibb9WQbvO8"},"outputs":[],"source":["# Define x --> (m, 1)\n","x = np.matrix([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n","m=x.shape[0]\n","# Define the number of coefficients to calculate (components of w --> w = (w0, w1, ..., wn))\n","num_coefficients = 4\n","\n","# Calculate phi\n","phi = np.power(x, range(0, num_coefficients))\n","print(\"Phi: \\n\\n{}\".format(phi))"]},{"cell_type":"markdown","metadata":{"id":"2eDN3Te4bvO8"},"source":["**2) Calculate its SVD**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJdmv3E0bvO9"},"outputs":[],"source":["U, S, Vt = np.linalg.svd(phi)\n","S = np.diag(S)\n","Sz= np.zeros((m,num_coefficients))\n","Sz[:num_coefficients,:num_coefficients]=S\n","S=Sz\n","\n","\n","print(\"U: \\n\\n{}\\n\\nS: \\n\\n{}\\n\\nVt: \\n\\n{}\".format(U, S, Vt))"]},{"cell_type":"markdown","metadata":{"id":"97HeOktLbvO9"},"source":["**3) Compute the pseudo-inverse of S**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cO3fXhEbvO-"},"outputs":[],"source":["# S is a diagonal matrix\n","S_pinv = np.linalg.pinv(S)\n","print(\"Pseudo-inverse of S: \\n\\n{}\".format(S_pinv))"]},{"cell_type":"markdown","metadata":{"id":"P8qr7j93bvO-"},"source":["**4) Calculate w\\***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w65SCfQabvO_"},"outputs":[],"source":["# Define y --> (m, 1)\n","y = np.matrix([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]) #same as x to w1=1 and wj=0 when j!=0\n","\n","# Compute w*\n","w_ast = Vt.T*S_pinv*U.T*y\n","print(\"W*\\n\\n{}\\n\".format(w_ast))\n","print(\"Is current w* equal to polyfit_inv_normal_eq output? {}\".format(np.allclose(w_ast, polyfit_inv_normal_eq(x, y, num_coefficients)[1])))"]},{"cell_type":"markdown","metadata":{"id":"FIo_DRK7bvO_"},"source":["**Function definition**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zsvNvAqbvO_"},"outputs":[],"source":["def polyfit_svd_normal_eq(x: np.matrix,\n","                          y: np.matrix,\n","                          n: int):\n","    \"\"\"\n","    Fits a polynomial of degree n to a sets of samples x and y. The polynomial minimizes the sum \n","    of squared errors (least squares) using the pseudoinverse of data matrix.\n","    \n","    :param x: m x 1, points in the x axis where the function is known\n","    :param y: m x 1, known values of the function at positions in x\n","    :param n: number of coefficients (degree of the polynomial + 1)\n","    \n","    :return w_ast: n x 1, vector of polynomial coefficients\n","    :return phi_pinv: n x m, pseudo-inverse of Phi\n","    :return phi: m x n, data matrix\n","    \"\"\"\n","    # TODO: Calculate phi\n","    phi = np.power(x, range(0, n))\n","    \n","    # TODO: Calculate its SVD\n","    U, S, Vt = np.linalg.svd(phi)\n","    S = np.diag(S)\n","    m=x.shape[0]\n","    Sz= np.zeros((m,n))\n","    p=min(m,n)\n","    Sz[:p,:p]=S\n","\n","    \n","    # TODO: Calculate the pseudo-inverse of S\n","    S_pinv = np.linalg.pinv(S)\n","    \n","    # TODO: Calculate w*\n","    phi_pinv = Vt.T*S_pinv*U.T\n","    w_ast = phi_pinv*y\n","    \n","    return w_ast, phi_pinv, phi"]},{"cell_type":"markdown","metadata":{"id":"dpYRBZNObvPA"},"source":["# 2. Principal Components Analysis\n","\n","#### 1. Complete the function pca_prin_dir for computing the p first principal directions via the eigenvectors of X<sup>T</sup>X.\n"]},{"cell_type":"markdown","metadata":{"id":"dtGJ-9kFbvPB"},"source":["**1) Center X**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1XHEswzbvPB"},"outputs":[],"source":["# Define x --> (m, n)\n","m=10\n","n=3\n","x = np.matrix(np.random.rand(m,n))\n","print(\"X: \\n\\n{}\\n\".format(x))\n","\n","# Calculate its mean\n","mu = np.mean(x, axis=0)\n","print(\"Mean: {}\\n\".format(mu))\n","\n","# Center X\n","x = x - mu\n","print(\"X centered: \\n\\n{}\\n\".format(x))"]},{"cell_type":"markdown","metadata":{"id":"yVSrtTTibvPB"},"source":["**2) Calculate the empirical covariance matrix**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xu1rJy2ubvPC"},"outputs":[],"source":["C = np.matmul(x.T, x) / (m - 1)\n","print(\"Covariance matrix: \\n\\n{}\".format(C))"]},{"cell_type":"markdown","metadata":{"id":"ARfjOYNAbvPC"},"source":["**3) Compute p-first principal directions**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1nPKNI53bvPC"},"outputs":[],"source":["# Define the number of p-directions to be retrieved\n","p = 1\n","\n","# Compute the principal directions, sort them by lambda values (eigen values) and return only \n","# the p-first values\n","eigen_values, eigen_vectors = np.linalg.eig(C)\n","print(\"Eigen values: \\n\\n{}\\n\\nEigen vectors: \\n\\n{}\".format(eigen_values, eigen_vectors))\n","idx = eigen_values.argsort()[::-1]\n","eigen_values = eigen_values[idx]\n","eigen_vectors = eigen_vectors[:,idx]    \n","eigen_values = eigen_values[:p]\n","eigen_vectors = eigen_vectors[:,:p]\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QcpDF7_bvPD"},"outputs":[],"source":["print(\"The p Eigen values: \\n\\n{}\\n\\nThe p Eigen vectors: \\n\\n{}\".format(eigen_values, eigen_vectors))"]},{"cell_type":"markdown","metadata":{"id":"7u0fPP80bvPD"},"source":["**Function definition**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7o4yTFj6bvPE"},"outputs":[],"source":["def pca_prin_dir(x: np.matrix,\n","                 p: int):\n","    \"\"\"\n","    Computes the principal directions, variances and mean of vectors in x. \n","    x is a row data matrix: its rows are vectors xi. The principal directions are given \n","    by eigenvectors of the empirical covariance matrix, x'*x.\n","        \n","    :param x: row data matrix, m x n\n","    :param p: number of principal directions\n","    \n","    :return eigen_vectors: principal direction matrix (each column is a PD) n x p\n","    :return eigen_values: eigenvalue diagonal matrix p x p\n","    :return mu: mean\n","    \"\"\"\n","    # TODO: Calculate the mean of the incoming data and center x\n","    mu = np.mean(x, axis=0)\n","    \n","    # TODO: Calculate the empirical covariance matrix\n","    C =  np.matmul(x.T, x) / (x.shape[0] - 1)\n","    \n","    # TODO: Compute the principal directions and sort them by lambda values (eigen values) and return only \n","    # the p-first values\n","    eigen_values, eigen_vectors = np.linalg.eig(C)\n","    idx = eigen_values.argsort()[::-1]\n","    eigen_values = eigen_values[idx][:p]\n","    eigen_vectors = eigen_vectors[:,idx][:, :p]\n","\n","\n","    return eigen_vectors, eigen_values, mu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gig24lNpbvPE"},"outputs":[],"source":["pca_prin_dir(x, p)"]},{"cell_type":"markdown","metadata":{"id":"ClUPeNyObvPF"},"source":["#### 2. Complete the function pca_prin_comp for computing the p first principal components [<x, v<sub>i</sub>>]<sub>i=1, ..., p</sub> of a point x\n"]},{"cell_type":"markdown","metadata":{"id":"BcZgMSb1bvPF"},"source":["- Equation to be used: z<sub>p</sub> = V<sub>p</sub><sup>T</sup>x --> z<sub>i</sub> = [<x<sub>i</sub>-mu,v<sub>1</sub>> <x<sub>i</sub>-mu,v<sub>2</sub>> ... <x<sub>i</sub>-mu,v<sub>p</sub>> ]\n","\n","\n","- Where: x = $\\hat{x}$ - &mu;"]},{"cell_type":"markdown","metadata":{"id":"WUQG1QxybvPG"},"source":["**1) Center X**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JJTgBCxvbvPG"},"outputs":[],"source":["# Define x --> (m, 1)same as before\n","m=10\n","n=2\n","x = np.zeros((m,n))\n","x[:,0] = np.linspace(0, 1, m)\n","x[:,1] = np.sin(3*np.pi*np.linspace(0, 3, m)/m)\n","print(\"X: \\n\\n{}\\n\".format(x))\n","\n","# Calculate its mean\n","mu = np.mean(x)\n","print(\"Mean: {}\\n\".format(mu))\n","\n","# Center X\n","x = x - mu\n","print(\"X centered: \\n\\n{}\\n\".format(x))"]},{"cell_type":"markdown","metadata":{"id":"8LyxI789bvPG"},"source":["**2) Project x over basis elements to compute principal components**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5iRr6YXCbvPH"},"outputs":[],"source":["\n","eigvalues, eigvect, mu = pca_prin_dir(x,p=2)\n","print(\"x shape: {}\".format(x.shape)), print(\"eigvect shape: {}\".format(eigvect.shape))\n","z_p = np.matmul(x, eigvect)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jswbw-pxbvPH"},"outputs":[],"source":["z_p"]},{"cell_type":"markdown","metadata":{"id":"PPO--ixEbvPI"},"source":["**Function definition**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FdNHgt2BbvPI"},"outputs":[],"source":["def pca_prin_comp(x: np.matrix,\n","                  eigen_vectors: np.matrix,\n","                  mu: np.matrix = None):\n","    \"\"\"\n","    Computes the principal components of vectors in x. x is a row data matrix: its rows are vectors xi. \n","    The principal components zi of a vector xi are given by the projection over the principal directions: \n","\n","                            zi = [ <xi-mu,v1> <xi-mu,v2> ... <xi-mu,vp> ]\n","\n","    :param x: row data matrix, m x n\n","    :param eigen_vectors: principal direction matrix (each column is a PD) n x p\n","\n","    :return z : m x p, principal component matrix. Row i contains the princpal components of xi\n","    \"\"\"\n","    # TODO: Calculate the raw data mean along row axis and center x\n","    if mu is None:\n","        mu = np.mean(x, axis=0)\n","    x = x - mu\n","    \n","    # Project x over basis elements to compute principal components\n","    z = np.matmul(x, eigen_vectors)\n","    \n","    return mu, z"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQz56o96bvPI"},"outputs":[],"source":["pca_prin_comp(x = x,\n","              eigen_vectors = eigvect)"]},{"cell_type":"markdown","metadata":{"id":"X1AFuvBhbvPJ"},"source":["#### 3. Complete the function pca_reconstruct for reconstructing a point x from its principal components.\n"]},{"cell_type":"markdown","metadata":{"id":"Z10diaB8bvPJ"},"source":["- Equation to be used: $\\hat{x}$ = z<sub>p</sub>V<sub>p</sub><sup>T</sup>\n","- Where: x = $\\hat{x}$ + &mu;\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqpJwRrebvPJ"},"outputs":[],"source":["# Calculate x_hat\n","z_p = np.array(z_p).reshape(-1, 1)\n","eigvect = np.array(eigvect).reshape(-1, 1)\n","print(\"z_p shape: {}\".format(z_p.shape)), print(\"eigvect shape: {}\".format(eigvect.shape))\n","x_hat = np.matmul(z_p, eigvect.T) \n","\n","# Add the mean to recover the real matrix values\n","x_ = x_hat + mu\n","print(x_)"]},{"cell_type":"markdown","metadata":{"id":"j19nj0HVbvPK"},"source":["**Function definition**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbXcSihqbvPK"},"outputs":[],"source":["def pca_reconstruct(z: np.matrix([]),\n","                    eigen_vectors: np.matrix([]),\n","                    mean: float):\n","    \"\"\"\n","    Given a set of vectors z(i,:) (rows of matrix z) expressed in principal components, this function computes the x(i,:), 'inverting' the PCA change of coordinates. \n","    It goes from the low dimensional PCA representation to the high dimensional vectors.\n","\n","    :param z: m x p, principal component matrix (row-wise). \n","    :param V: principal direction matrix (each column is a PD) n x p\n","    :param mu: mean\n","\n","    :return x: row data matrix, m x n. Row x(i,:) is the high dimensional reconstruction from z(i,:)\n","    \"\"\"\n","    # Calculate x_hat\n","    x_hat = np.matrix([])\n","\n","    # Add the mean to recover the real matrix values\n","    x_ = np.matrix([])\n","    \n","    return x_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUnljSOSbvPK"},"outputs":[],"source":["# z_p is calculated in the step before\n","z_p = np.matrix([]) # --> Comment/delete this line when the above one is run\n","pca_reconstruct(z = z_p, eigen_vectors = eigen_vectors, mean = mu)"]},{"cell_type":"markdown","metadata":{"id":"ZfJCSKGzbvPL"},"source":["#### 4. For the flat ellipsoid dataset (provided together with the code) run the following lines. Four figures will open. Read the code and explain what each figure is showing. Answer the questions asked in the code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6ssVVNJbvPL"},"outputs":[],"source":["# Install a widget that will allow you to build up interactive plots\n","# !conda install -c conda-forge ipympl -y\n","\n","# If using JupyterLab\n","# !conda install -c conda-forge nodejs -y\n","# !jupyter labextension install @jupyter-widgets/jupyterlab-manager jupyter-matplotlib\n","\n","# After run this lines, close the jupyter session and restore it"]},{"cell_type":"markdown","metadata":{"id":"pzBrdsgUbvPL"},"source":["**1) Read the CSV file which contains the data and plot them**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1pJn1HSi_qU"},"outputs":[],"source":["#pip install mpld3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjYfngnmjBCb"},"outputs":[],"source":["#pip install ipympl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4sg5aM9bvPM"},"outputs":[],"source":["# Import Pandas --> see https://pandas.pydata.org/ \n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import mpld3\n","import matplotlib.cm as cm\n","%matplotlib widget"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_MIjbVAbvPM"},"outputs":[],"source":["# Read the file\n","data = pd.read_csv(\"../data/point_cloud_r3.csv\", sep = \",\", header=None, names = [\"x\", \"y\", \"z\"])\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVP0HNpjbvPN"},"outputs":[],"source":["# Plot the data\n","fig = plt.figure(figsize=(8, 6))\n","ax = fig.add_subplot(111, projection='3d', )\n","ax.scatter(data['x'], data['y'], data['z'], s=10, alpha=0.9, edgecolors='w', depthshade = False)\n","plt.xticks(list(range(int(data['x'].min()), int(data['x'].max()), 1)))\n","plt.yticks(list(range(int(data['y'].min()), int(data['y'].max()), 1)))\n","ax.set_zticks(list(range(int(data['z'].min()), int(data['z'].max()), 1)))\n","\n","ax.set_xlabel('X')\n","ax.set_ylabel('Y')\n","ax.set_zlabel('Z')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Gdd5cVxsbvPN"},"source":["**2) Compute principal all principal directions and the correspoding eigenvalues**\n","\n","**NOTE:** in this example we are going to do some visualization of the principal\n","components in IR^3. For that we will compute ALL principal directions and ALL\n","principal components. Please note that this is not the typical way in which\n","PCA is used.  In a normal application of dimensionality reduction we would\n","compute only a few principal directions and principal components."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ebaKfPkxbvPN"},"outputs":[],"source":["# Define the number of principal directions \n","p = len(data.columns)\n","\n","# Calculate the PCA principal directions for the given data\n","eigen_vectors, eigen_values, mu = pca_prin_dir(data.values, p)\n","print(\"Eigen values: \\n\\n{}\\n\\nEigen vectors: \\n\\n{}\".format(eigen_values, eigen_vectors))"]},{"cell_type":"markdown","metadata":{"id":"zubCq-YKbvPO"},"source":["#### Answer the following questions:\n","**a) What can you tell from the eigenvalues about the geometry of the original point set?**\n","\n","Answer: \n","\n","**b) Which is the number of components you would keep to hold at least the 95% of the variance?**\n","\n","In order to answer this question, we must represent the explained variance in terms of participation. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vSlKm2iJbvPO"},"outputs":[],"source":["# Calculate the participation in terms of %\n","tot = sum(eigen_values)\n","var_exp = [(i / tot)*100 for i in sorted(eigen_values, reverse=True)]\n","cum_var_exp = np.cumsum(var_exp)\n","\n","# Plot the graphic\n","with plt.style.context('seaborn-whitegrid'):\n","    plt.figure(figsize=(6, 4))\n","\n","    plt.bar(range(len(eigen_values)), var_exp, alpha=0.5, align='center',\n","            label='individual explained variance')\n","    plt.step(range(len(eigen_values)), cum_var_exp, where='mid',\n","             label='cumulative explained variance')\n","    plt.ylabel('Explained variance ratio')\n","    plt.xlabel('Principal components')\n","    plt.legend(loc='best')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"izmPCYPlbvPP"},"source":["Answer: "]},{"cell_type":"markdown","metadata":{"id":"nM4wtxS0bvPP"},"source":["**c) How large would be the mean projection error we would commit by keeping only the first principal component?**\n","\n","In theory, MSE and eigenvalues are related by the following equation:\n","\n","\\begin{equation*}\n","\\frac{1}{m}\\sum^{n}_{i=1}\\|x_{i}-P_{V_{p}}(x_{i})\\|^{2}= \\sum^{n}_{j=p+1}\\lambda_{j}\n","\\end{equation*}\n","\n","Therefore, we will check that in both cases we achieve the same result."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlHIUZz3bvPP"},"outputs":[],"source":["# Define the number of components to be kept\n","components = 2\n","components_array = list(range(components))\n","\n","# Calculate the projection of x over the eigenvectors basis by only keeping the first component\n","mu, z = pca_prin_comp(x = data.values,\n","                      eigen_vectors = eigen_vectors)\n","projection = pca_reconstruct(z = z[:, components_array], eigen_vectors = eigen_vectors[:, components_array], mean = mu)\n","\n","# Calculate the MSE\n","print(\"MSE for the first {} components: {}\".format(components, np.power(data.values - projection, 2).sum() / (len(data) - 1)))"]},{"cell_type":"markdown","metadata":{"id":"Q_SsT70lbvPQ"},"source":["**Function definition**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2kP4gvC7bvPQ"},"outputs":[],"source":["def calculate_projection_mse(x: np.matrix,\n","                             eigen_vectors: np.matrix,\n","                             z: np.matrix,\n","                             p: np.matrix,\n","                             mean: float):\n","    \"\"\"\n","    Calculates the projection MSE\n","    \n","    :param x: row data matrix, m x n\n","    :param eigen_vectors: principal direction matrix (each column is a PD) n x p\n","    :param z: m x p, principal component matrix. Row i contains the princpal components of xi    \n","    :param p: number of components to be used when projecting\n","    :param mu: mean\n","    \n","    :return:\n","    \"\"\"\n","    # Define the number of components to be kept\n","    components_array = list(range(p))\n","\n","    # Calculate the projection of x over the eigenvectors basis by only keeping the first component\n","    mu, z = pca_prin_comp(x = x,\n","                          eigen_vectors = eigen_vectors)\n","    projection = pca_reconstruct(z = z[:, components_array], eigen_vectors = eigen_vectors[:, components_array], mean = mean)\n","\n","    # Calculate the MSE\n","    mse = np.power(x - projection, 2).sum() / (len(x) - 1)\n","    \n","    return mse"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRj1mhNVbvPQ"},"outputs":[],"source":["components = 1\n","print(\"Shape of z:\", z.shape)\n","print(\"MSE for the first {} components: {}\".format(components, calculate_projection_mse(x = data.values,\n","                                                                                        eigen_vectors = eigen_vectors,\n","                                                                                        z = z,\n","                                                                                        p = components,\n","                                                                                        mean = mu)))"]},{"cell_type":"markdown","metadata":{"id":"ulBA4akObvPR"},"source":["**d) And if we keep the first and the second?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UOIgnEsbvPR"},"outputs":[],"source":["components = 2\n","print(\"MSE for the first {} components: {}\".format(components, calculate_projection_mse(x = data.values,\n","                                                                                        eigen_vectors = eigen_vectors,\n","                                                                                        z = z,\n","                                                                                        p = components,\n","                                                                                        mean = mu)))"]},{"cell_type":"markdown","metadata":{"id":"yKTK9AtLbvPR"},"source":["**3) Now plot the point cloud in 3D. Do three plots, in each of them will color the points with the value of one principal component (use the provided function plot_3d_and_components). Also plot the principal directions as vectors at the origin. The length of these vectors is proportional to the corresponding eigenvalue lambda.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7QYb95SvbvPS"},"outputs":[],"source":["def plot_3d_and_components(xs: np.matrix,\n","                           ys: np.matrix,\n","                           zs: np.matrix,\n","                           means: np.matrix,\n","                           principal_components: np.matrix,\n","                           p: int,\n","                           eigen_vectors: np.matrix = None,\n","                           color_map: list = []):\n","    \"\"\"\n","    Plots a 3D graph and the PCA components\n","    \n","    :param xs: x-axis data\n","    :param ys: y-axis data\n","    :param zs: z-axis data\n","    :param means: means per axis\n","    :param eigen_vectors: eigenvectors\n","    :param principal_components: principal component matrix\n","    :param p: number of components to use\n","    :param color_map: color map to be used when representing\n","    \"\"\"\n","    # Import libraries\n","    import matplotlib\n","    import inflect\n","\n","    # Calculate the maximum and minimum values\n","    components = list(range(p))\n","    minimas = principal_components[:, components].min(axis = 0)\n","    maximas = principal_components[:, components].max(axis = 0)\n","    \n","    # Build up a object to convert from number to ordinal \n","    p = inflect.engine()\n","\n","    # Build up the colors map and plot the figures\n","    fig = plt.figure(figsize=(10, 8))\n","    xticks = list(range(int(xs.min()), int(xs.max()), 1))\n","    yticks = list(range(int(ys.min()), int(ys.max()), 1))\n","    zticks = list(range(int(zs.min()), int(zs.max()), 1))\n","    for component in components:\n","        norm = matplotlib.colors.Normalize(vmin=minimas[component], vmax=maximas[component], clip=True)\n","        mapper = cm.ScalarMappable(norm=norm, cmap=color_map[component])\n","        color = [mapper.to_rgba(v)[0] for v in z[:, [component]]]\n","        \n","        # Plot the data\n","        ax = fig.add_subplot(1, len(components), component + 1, projection='3d')\n","        ax.scatter(xs, ys, zs, s=10, c = color, alpha=0.9, edgecolors='w', depthshade = False)\n","        if eigen_vectors is not None:\n","            for v in eigen_vectors.T:\n","                ax.plot([means[0] - v[0], v[0] + means[0]], \n","                        [means[1] - v[1], v[1] + means[1]],\n","                        [means[2] - v[2], v[2] + means[2]], \n","                        linewidth = 3,\n","                        color = \"darkblue\")\n","        ax.set_xticks(xticks)\n","        ax.set_yticks(yticks)\n","        ax.set_zticks(zticks)\n","        ax.set_xlabel('X')\n","        ax.set_ylabel('Y')\n","        ax.set_zlabel('Z')\n","        ax.set_title(\"Principal directions and {} principal component (color).\".format(p.ordinal(component + 1)), fontdict = {\"fontsize\": 6})\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHul_9GVbvPS"},"outputs":[],"source":["plot_3d_and_components(xs = data.x,\n","                       ys = data.y,\n","                       zs = data.z,\n","                       means = mu,\n","                       eigen_vectors = eigen_vectors * eigen_values * 0.5,\n","                       principal_components = z,\n","                       p = 3,\n","                       color_map = [cm.gnuplot, cm.gnuplot, cm.gnuplot])"]},{"cell_type":"markdown","metadata":{"id":"HvLQIGd9bvPT"},"source":["**4) Now let's do some dimensionality reduction. For that, we keep some of theprincipal components z.**\n","\n","**a) Keep the first two principal components, project the points into a two dimensional vector space and calculate the projection error.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6sTo5-hrbvPT"},"outputs":[],"source":["def plot_3d_and_projections(xs: np.matrix,\n","                            ys: np.matrix,\n","                            zs: np.matrix,\n","                            xp: np.matrix,\n","                            yp: np.matrix,\n","                            zp: np.matrix,\n","                            title: str = \"\"):\n","    \"\"\"\n","    Plots a 3D graph and the PCA components\n","    \n","    :param xs: x-axis data\n","    :param ys: y-axis data\n","    :param zs: z-axis data\n","    :param xp: x-axis projection data\n","    :param yp: y-axis projection data\n","    :param zp: z-axis projection data\n","    \"\"\"\n","    # Import libraries\n","    import matplotlib\n","\n","    # Build up the colors map and plot the figures\n","    fig = plt.figure(figsize=(10, 8))\n","    xticks = list(range(int(xs.min()), int(xs.max()), 1))\n","    yticks = list(range(int(ys.min()), int(ys.max()), 1))\n","    zticks = list(range(int(zs.min()), int(zs.max()), 1))\n","    \n","    # Plot the data\n","    ax = fig.add_subplot(1, 1, 1, projection='3d')\n","    ax.scatter(xs, ys, zs, s=10, c = \"darkblue\", alpha=0.9, edgecolors='w', depthshade = False)\n","    ax.scatter(xp, yp, zp, s=10, c = \"red\", alpha=0.9, edgecolors='w', depthshade = False)\n","    ax.set_xticks(xticks)\n","    ax.set_yticks(yticks)\n","    ax.set_zticks(zticks)\n","    ax.set_xlabel('X')\n","    ax.set_ylabel('Y')\n","    ax.set_zlabel('Z')\n","    ax.set_title(title, fontdict = {\"fontsize\": 6})\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"o_Xn_DLtbvPT"},"source":["First and second components"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QPrwHsi1bvPU"},"outputs":[],"source":["# Calculate the projection of x over the eigenvectors basis by only keeping the first and second components\n","components_array = [0, 1]\n","mu, z = pca_prin_comp(x = data.values,\n","                      eigen_vectors = eigen_vectors)\n","projection = pca_reconstruct(z = z[:, components_array], eigen_vectors = eigen_vectors[:, components_array], mean = mu)\n","error_projection_fs = np.sqrt(np.power(data.values - projection, 2).sum(axis = 1))\n","\n","# Plot the projection\n","plot_3d_and_projections(xs = data.x,\n","                        ys = data.y,\n","                        zs = data.z,\n","                        xp = projection[:, 0],\n","                        yp = projection[:, 1],\n","                        zp = projection[:, 2],\n","                        title = \"Real data versus projection over the first and second components\")"]},{"cell_type":"markdown","metadata":{"id":"BuPLXXosbvPU"},"source":["**b) Keep the first and third principal components, project the points into a two dimensional vector space and calculate the projection error.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiuoaOI-bvPU"},"outputs":[],"source":["# Calculate the projection of x over the eigenvectors basis by only keeping the first and second components\n","components_array = [0, 2]\n","mu, z = pca_prin_comp(x = data.values,\n","                      eigen_vectors = eigen_vectors)\n","projection = pca_reconstruct(z = z[:, components_array], eigen_vectors = eigen_vectors[:, components_array], mean = mu)\n","error_projection_ft = np.sqrt(np.power(data.values - projection, 2).sum(axis = 1))\n","\n","# Plot the projection\n","plot_3d_and_projections(xs = data.x,\n","                        ys = data.y,\n","                        zs = data.z,\n","                        xp = projection[:, 0],\n","                        yp = projection[:, 1],\n","                        zp = projection[:, 2],\n","                        title = \"Real data versus projection over the first and third components\")"]},{"cell_type":"markdown","metadata":{"id":"qwuUQU_tbvPV"},"source":["**c) Which 2D subspace approximates better the original points?**\n","\n","Answer: "]},{"cell_type":"markdown","metadata":{"id":"lB7zVO9FbvPV"},"source":["**5) Plot the low dimensional representative z in IR^2 and color them with the projection error. Look at the colorbars to compare the error in both plots.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZESDydEFbvPV"},"outputs":[],"source":["fig, (ax1, ax2) = plt.subplots(figsize=(10, 8), ncols=2)\n","sc = ax1.scatter(z[:, 0] , z[:, 1], s=10, c = error_projection_fs, alpha=0.9, edgecolors='w')\n","plt.colorbar(sc, ax = ax1)\n","ax1.set_title(\"First and second Principal Components\")\n","sc = ax2.scatter(z[:, 0] , z[:, 2], s=10, c = error_projection_ft, alpha=0.9, edgecolors='w')\n","plt.colorbar(sc, ax = ax2)\n","ax2.set_title(\"First and third Principal Components\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yxQwWeSjbvPW"},"source":["#### 5. This is the first script of the face recognition application. The idea here is to run a very simple face recognition algorithm, but using PCA to reduce the dimensionality of the vectors. The original vectors are images of 211 x 229. If we consider the images as vectors in IR^n (each pixel is a component of the vector), we have that n = 221x229 = 48319. Using PCA, we will reduce the dimensionality to ~ 50, 1000 times smaller!!!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pH5hqZilbvPW"},"outputs":[],"source":["# In case needed:\n","#!conda install -c anaconda pillow -y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_8GyL2fbvPW"},"outputs":[],"source":["from PIL import Image\n","import os"]},{"cell_type":"markdown","metadata":{"id":"wB4fYFpsbvPX"},"source":["**1) Load faces from image files** \n","\n","Images are in data/faces folder. We will load them and store them as rows in a data matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i1THJsfibvPX"},"outputs":[],"source":["# List all images and read them. Once read, reshape them into (1, W x H)\n","faces_dir = os.path.abspath(\"../data/faces\")\n","faces_files = [file_ for file_ in os.listdir(faces_dir) if file_.endswith(\"png\")]\n","faces_shape = np.array(Image.open(os.path.join(faces_dir, faces_files[0]))).shape\n","faces = np.empty((len(faces_files), faces_shape[0] * faces_shape[1])) \n","for index, img in enumerate(faces_files):\n","    im_frame = Image.open(os.path.join(faces_dir, img))\n","    np_frame = np.array(im_frame)\n","    faces[index] = np_frame.reshape((1, faces_shape[0] * faces_shape[1]))"]},{"cell_type":"markdown","metadata":{"id":"moq_ihRnbvPY"},"source":["**2) Split data randomly into a train set (tr), and a test set (ts)**\n","\n","Leave a part of the data set to test. The test data set are faces we will try to recognize.\n","\n","The sizes will be 80% for training and 20% for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYdRL1KkbvPY"},"outputs":[],"source":["import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FlUipRc1bvPY"},"outputs":[],"source":["# Get the training and testing faces indexes\n","seed_value = None  \n","np.random.seed(seed_value)\n","test_faces_idx = random.sample(list(range(len(faces))), k = int(len(faces) * 0.205))\n","train_faces_idx = list(set(list(range(len(faces)))).difference(set(test_faces_idx)))\n","\n","# Build up the training and testing faces\n","test_faces = faces[test_faces_idx, :]\n","train_faces = faces[train_faces_idx, :]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7zJTZ9tbvPY"},"outputs":[],"source":["len(train_faces)"]},{"cell_type":"markdown","metadata":{"id":"rVbZei8WbvPZ"},"source":["**3) Train PCA model**\n","\n","Now we will compute the principal directions using the training set. \n","\n","**Note:** the covariance matrix is too large to fit in memory. We can circumvent this problem using the SVD of x. Recall that the eigenvectors of x'\\*x correspond to the right singular vectors (columns of V):\n","\n","x = U\\*S\\*V'\n","\n","x'\\*x = V\\*S'\\*U'\\*U\\*S\\*V' = V\\*S'\\*S\\*V' \n","\n","However, this does not work either. We have the same problem! The size of V does not fit in memory. This is not surprising, both C and V are n x n matrices. \n","\n","We can solve this problem with the economic size SVD. Note that n - m columns of V are multiplied by the zeros of S, in the product U\\*S\\*V'. This means that we can remove this columns. The economic size SVD is the following: \n","\n","x = U\\*S0\\*V0'\n","\n","where S0 is m x m and V0 is n x m. The same can be done with the U matrix in the case m > n.\n","\n","We want to reduce V, not U. We can solve this problem very easily, by computing the svd of x'. It can be shown that if\n","\n","x = U\\*S\\*V'\n","\n","is the SVD of x, then the svd of x' is given by\n","\n","x' = V\\*S'\\*U\n","\n","The same happens with the economic size svd. Thus we will compute V0 via the economic svd of x'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_HMpXzkbvPZ"},"outputs":[],"source":["# Define P\n","p = len(train_faces)\n","\n","# Calculate the SVD of x' (x = x - mean(x))\n","U, S, Vt = np.linalg.svd((train_faces - train_faces.mean(axis = 0)).T, full_matrices=False)\n","\n","# Keep only the first p \n","V  = U[:, :p]\n","S = np.diag(S)[:p, :p]\n","\n","# The eigenvalues of x'*x are the square of the singular values\n","S = np.power(S, 2)"]},{"cell_type":"markdown","metadata":{"id":"N02oUikQbvPa"},"source":["**4) Visualize the PCA results**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fh0PLaoybvPa"},"outputs":[],"source":["# Calculate the participation in terms of %\n","tot = sum(np.diag(S))\n","var_exp = [(i / tot)*100 for i in sorted(np.diag(S), reverse=True)]\n","cum_var_exp = np.cumsum(var_exp)\n","\n","# Plot the graph\n","with plt.style.context('seaborn-whitegrid'):\n","    plt.figure(figsize=(8, 6))\n","    plt.bar(range(len(np.diag(S))), var_exp, alpha=0.5, align='center',\n","            label='individual explained variance')\n","    plt.step(range(len(np.diag(S))), cum_var_exp, where='mid',\n","             label='cumulative explained variance')\n","    plt.ylabel('Explained variance ratio')\n","    plt.xlabel('Principal components')\n","    plt.legend(loc='best')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"evREqTcJbvPa"},"source":["As you can see in graph above, by using the first 50 principal components we can keep almost the 90% of the total variance. Let's take a look at the *mean face*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EoTOSDFbvPb"},"outputs":[],"source":["# Define the number of principal components to keep\n","p = 50\n","\n","# Keep only the first p principal directions\n","V_p = V[:, :p]\n","S_p = S[:p, :p]\n","\n","# Plot the mean face\n","plt.figure()\n","plt.imshow(train_faces.mean(axis = 0).reshape(faces_shape), cmap = \"gray\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"c9d_nzTgbvPb"},"source":["And now build a big image with the 6 first principal directions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuObXatLbvPb"},"outputs":[],"source":["# Build up the images matrix\n","num_rows = 2\n","num_columns = 3\n","tmp = np.empty(shape = (faces_shape[0] * num_rows, faces_shape[1] * num_columns))\n","component_index = 0\n","for row in range(num_rows):\n","    for column in range(num_columns):\n","        tmp[faces_shape[0] * row: faces_shape[0] * (row + 1), faces_shape[1] * column: faces_shape[1] * (column + 1)] = (S_p[component_index, component_index] * V_p[:, component_index]).reshape(faces_shape)\n","        component_index += 1\n","\n","# Plot the images\n","plt.figure()\n","plt.imshow(tmp, cmap = \"gray\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JRVgUegYbvPc"},"source":["**5) project a test face over principal directions**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SHdj_URbvPc"},"outputs":[],"source":["# Keep one of the test faces\n","x = test_faces[0, :]\n","\n","# Build up the principal components\n","_, z = pca_prin_comp(x = x, eigen_vectors = V_p, mu = train_faces.mean(axis = 0)); \n","\n","# Reconstruct the face\n","x_proj = pca_reconstruct(z = z, eigen_vectors = V_p, mean = train_faces.mean(axis = 0))\n","\n","# Show both faces\n","fig, (ax1, ax2) = plt.subplots(figsize=(8, 6), ncols=2)\n","ax1.imshow(x.reshape(faces_shape), cmap = \"gray\")\n","ax1.set_title(\"Test image\")\n","ax2.imshow(x_proj.reshape(faces_shape), cmap = \"gray\")\n","ax2.set_title(\"Projected image\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iIa3ONkmbvPc"},"source":["**6) Recognize faces**\n","\n","This a very simple face recognition algorithm: it is based on comparing the PCA coordinages of a query face with those of the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RptUooNQbvPd"},"outputs":[],"source":["# Import a library to simulate time steps\n","import time\n","%matplotlib inline\n","\n","# Define the number of principal components to keep\n","p = 50\n","\n","# Keep only the first p principal directions\n","V_p = V[:, :p]\n","S_p = S[:p, :p]\n","\n","# Compute principal components for training faces\n","_, train_z = pca_prin_comp(x = train_faces, eigen_vectors = V_p, mu = train_faces.mean(axis = 0))\n","\n","# Compute principal components for test faces\n","_, test_z = pca_prin_comp(x = test_faces, eigen_vectors = V_p, mu = train_faces.mean(axis = 0))\n","\n","# Classify\n","nn_idx = []\n","for i in range(len(test_faces)):\n","    d = np.linalg.norm(np.tile(test_z[[i], :], (train_z.shape[0], 1)).T - train_z.T, axis = 0).T\n","    nn_idx.append(int(d.argmin()))\n","\n","# Visualize classification results\n","plt.figure()\n","plt.ion()\n","for i in range(len(test_faces)):\n","    print(\"Showing image {} of {}\".format(i + 1, len(test_faces)))\n","    plt.subplot(1, 2, 1).imshow(test_faces[i, :].reshape(faces_shape), cmap = \"gray\")\n","    ax1.set_title(\"Test image\")\n","    plt.subplot(1, 2, 2).imshow(train_faces[nn_idx[i], :].reshape(faces_shape), cmap = \"gray\")\n","    ax2.set_title(\"Training image\")\n","    plt.show()\n","    plt.pause(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7LxRVcwbvPd"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["NCgJoqfCbvO7"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
