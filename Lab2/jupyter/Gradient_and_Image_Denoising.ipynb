{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"JM_ErGq-k_3c"},"source":["# Group Members:\n","\n","*   Name 1\n","*   Name 2\n","*   Name 3"]},{"cell_type":"markdown","metadata":{"id":"sf3OE-KdlByS"},"source":["# Lab 2 Assignment"]},{"cell_type":"code","metadata":{"id":"rM-59dldGpkD"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JpkW3zxtGo0f"},"source":["%cd \"PUT_YOUR_PATH\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h-tLxR9EGlep"},"source":["# **Practical session 2: an image denoising energy**"]},{"cell_type":"markdown","metadata":{"id":"ndG1DGqaGlex"},"source":["## **1. Gradient descent**"]},{"cell_type":"code","metadata":{"id":"K005GW8yGlex"},"source":["import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import os\n","\n","from mpl_toolkits.mplot3d import Axes3D\n","from matplotlib import cm\n","from IPython import display"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s_58fnA6Gley"},"source":["### **1. Complete the MatLab functions toy_fun and toy_gradient. These functions implement the function f and its gradient.**"]},{"cell_type":"markdown","metadata":{"id":"S1VqvDiDGlez"},"source":["**toy_fun**"]},{"cell_type":"markdown","metadata":{"id":"04Zac59DU-Dp"},"source":["\\begin{equation}\n","f(x_1,x_2) = \\frac1{1000}\\left(x_1^4 + x_2^4 - 80 x_1^2 - 60 x_2^2 + 100x_1 +\n","50 x_2 + 1\\right)\n","\\end{equation}"]},{"cell_type":"code","metadata":{"id":"iu4TQq6cGlez"},"source":["def toy_fun(x: np.matrix):\n","    \"\"\"\n","    Polynomial toy function - see the guide\n","    \n","    :param x: list [x_1, x_2] \n","    \n","    :return y: value of the function at point x\n","    \"\"\"\n","    # TODO: Compute the function\n","    return 0.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["toy_fun([100,2])"],"metadata":{"id":"RYfobzAISRUf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gNg3oY61Gle0"},"source":["**toy_gradient**"]},{"cell_type":"markdown","source":["### **Gradient Descent Recap**\n","\n","Given a function $f(x)$:\n","\n","Set:\n","+ initial/starting point $x^{0}$\n","+ step size $\\eta$\n","+ tolerance $\\tau$\n","+ maximum number of iterations $K$\n","\n","\n","At iteration $k=0,1,2,\\cdots K$,\n","\n","1.   Compute:\n","$$\\nabla_{x} f(x^{k})$$\n","2.   Take $\\eta$ steps in the direction of steepest descent:\n","$$x^{k+1} = x^{k} - \\eta\\nabla_{x} f(x^{k})$$\n","3.   Repeat 1 and 2 until $\\|\\nabla_{x} f(x^{k})\\|_{2}\\leq \\tau$ or the maximum number of iterations has been exceeded\n","\n","\n","\n","\n","\n","$x$, $x^{0}\\in \\mathbb{R}^{n}$"],"metadata":{"id":"auh6FMZ7Mfp0"}},{"cell_type":"markdown","metadata":{"id":"jLA5PY1VVHWA"},"source":["We know that:\n","\n","$$\\nabla f(x_{1},x_{2}) =\n","\\begin{bmatrix}\n","\\dfrac{\\partial f(x_{1},x_{2})}{\\partial x_{1}}\\\\\n","\\\\\n","\\dfrac{\\partial f(x_{1},x_{2})}{\\partial x_{2}}\n","\\\\\n","\\end{bmatrix} =\n","\\begin{bmatrix}\n","\\frac1{1000}\\left(4x_1^3 - 160 x_1 + 100\\right)\\\\\n","\\\\\n","\\frac1{1000}\\left(4x_2^3 - 120 x_2 + 50\\right)\n","\\\\\n","\\end{bmatrix}$$"]},{"cell_type":"code","metadata":{"id":"DmqASbCOGle0"},"source":["def toy_gradient(x: np.matrix):\n","    \"\"\"\n","    Gradient of toy_fun polynomial toy function \n","    \n","    :param x: 2x1 matrix \n","    :return grad: 2x1 matrix: gradient of the toy function at point x\n","    \"\"\"\n","    # TODO: Compute the gradient of the toy function (must be calculated by hand)\n","    return np.matrix([])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = np.matrix([[1,2]]).T\n","toy_gradient(x)"],"metadata":{"id":"JzT5LlzgSVpl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTGNASsYGle1"},"source":["### **2. Complete the MatLab function gradient_descent. This function implements a gradient descent algorithm. We are going to implement it in a way in which we can use the same gradient descent function for this toy example and for the denoising energy of the next section. Follow the comments provided in the code.**"]},{"cell_type":"code","metadata":{"id":"I1ZgrgcRGle1"},"source":["def gradient_descent(callback,\n","                     callback_params: dict, \n","                     initial_condition: np.matrix, \n","                     step_size: float,\n","                     max_iterations: int,\n","                     tolerance: float,\n","                     fig = None,\n","                     ax = None):\n","    \"\"\"\n","    Implementation of the gradient descent algorithm with\n","    fixed step size. It uses function handles (handles are MatLab pointers). It\n","    can work with any function and gradient if they are implemented with . Here params\n","    is a structure with the internal parameters of my_fun and my_grad.\n","\n","    :param callback: gradient of function to be optimized\n","    :param callback_params: a structure with the internal parameters of the target function and \n","                            its gradient. Useful for the image denoising task.\n","    :param initial_condition: initial condition for gradient descent\n","    :param step_size: size of the gradient descent steps\n","    :param max_iterations: maximum number of iterations\n","    :param tolerance: tolerance for the stopping condition (it stop when \n","                      the norm of the gradient is below the tolerance)\n","\n","    :return current_value: value found\n","    \"\"\"\n","    # Initialize variables\n","    current_value = initial_condition\n","    previous_value = current_value\n","    current_iteration = 0\n","    current_norm_value = np.inf\n","    \n","    # Main loop for Gradient Descent\n","    while (current_norm_value > tolerance) and (current_iteration < max_iterations):\n","        # Keep previous - just for visualization\n","        previous_value = current_value\n","\n","        # TODO: Run the gradient descent\n","        gf = np.matrix([])\n","        \n","        # TODO: Update the current value and norm value of gradient\n","        current_value = np.matrix([])\n","        current_norm_value = np.matrix([])\n","        print(\"{} of {} -> tolerance: {}\".format(current_iteration, max_iterations, current_norm_value))\n","        \n","        # Plot current position! Just for visualization purposes \n","        # if x is a 2x1 vector (visualization of toy example) \n","        if (current_value.shape[0] == 2) & (current_value.shape[1] == 1):\n","            if not ax:\n","                fig, ax = plt.subplots()\n","            ax.plot(current_value[1, 0], current_value[0, 0], marker = 'o', color = \"k\")\n","            ax.plot([previous_value[1, 0], current_value[1, 0]], \n","                    [previous_value[0, 0], current_value[0, 0]], \"-k\")\n","            display.clear_output(wait=True)\n","            display.display(fig)            \n","\n","        # Update the iteration\n","        current_iteration += 1\n","        \n","    return current_value"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fYVlFqz2Gle3"},"source":["### **3 Run the following block and answer the questions on the PDF**"]},{"cell_type":"code","metadata":{"id":"wCyMh-1jGle3"},"source":["def toy_main():\n","    # Set the grids\n","    x1 = np.arange(-10, 10, 0.1)\n","    x2 = np.arange(-10, 10, 0.1)\n","\n","    # Evaluate the toy_fun\n","    y = np.zeros(shape = (len(x1), len(x2)))\n","    for i, x1_value in enumerate(x1):\n","        for j, x2_value in enumerate(x2):\n","            y[i, j] = toy_fun([x1_value, x2_value])\n","\n","    # Plot the surface.\n","    fig = plt.figure(figsize = (14, 8))\n","    # ax = fig.gca(projection='3d')\n","    ax = fig.add_subplot(projection='3d')\n","    X, Y = np.meshgrid(x1, x2)\n","    Z = y\n","    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n","                           linewidth=0, antialiased=False)\n","\n","    # Add a color bar which maps values to colors.\n","    fig.colorbar(surf, shrink=0.5, aspect=5)\n","\n","    # Show the surface\n","    #plt.show()\n","\n","    # Plot the contours\n","    fig_contours, ax_contours = plt.subplots(figsize = (10, 8))\n","    CS = ax_contours.contour(X, Y, Z, \n","                    corner_mask = False, levels = 150, \n","                    linewidths=(1,), cmap = cm.coolwarm)\n","    \n","    # Set initial condition --> TRY CHANGING IT\n","    x0 = np.matrix([[5], [-5]])\n","\n","    # Set gradient descent parameters --> EXPLORE USING DIFFERENT PARAMETERS\n","    step_size= 1\n","    tolerance = 0.01\n","    max_iterations = 200\n","\n","    # Call gradient descent minimization\n","    print(\"First gradient descent ...\")\n","    xs_1 = gradient_descent(callback = toy_gradient,\n","                            callback_params = {}, \n","                            initial_condition = x0, \n","                            step_size = step_size,\n","                            max_iterations = max_iterations,\n","                            tolerance = tolerance,\n","                            fig = fig_contours,\n","                            ax = ax_contours)\n","\n","    # Set gradient descent parameters --> EXPLORE USING DIFFERENT PARAMETERS\n","    step_size= 5\n","    tolerance = 0.1\n","    max_iterations = 100\n","\n","    # Call gradient descent minimization\n","    print(\"\\nSecond gradient descent ...\")\n","    xs_2 = gradient_descent(callback = toy_gradient,\n","                            callback_params = {}, \n","                            initial_condition = x0, \n","                            step_size = step_size,\n","                            max_iterations = max_iterations,\n","                            tolerance = tolerance,\n","                            fig = fig_contours,\n","                            ax = ax_contours)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6onnRAWGle5"},"source":["toy_main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v8Dik7oYGle6"},"source":["## **2. Image denoising energy**"]},{"cell_type":"markdown","metadata":{"id":"Pd5F9np2Gle6"},"source":["### **1. Complete the functions _im_fwd_gradient_ and _im_bwd_divergence_. These functions compute the forward gradient &nabla;<sup>+</sup> and the backwards divergence div<sup>-</sup>. Follow the comments provided in the code. Avoid building the matrices &nabla;<sup>+</sup> and div<sup>-</sup>**"]},{"cell_type":"markdown","metadata":{"id":"rNOgE1ZbGle6"},"source":["#### **1.1 im_fwd_gradient**"]},{"cell_type":"markdown","metadata":{"id":"LQh9TUgD1C1S"},"source":["For matrix $u$, $\\nabla^{+}u_{i,j} = (\\nabla^{+}_{i}u_{i,j},\\nabla^{+}_{j}u_{i,j})$ where:\n","\n","$$\\nabla^{+}_{i}u_{i,j} = \\begin{cases}\n","  u_{i+1,j} - u_{i,j} & \\text{if }i<M\\\\ \n","  0& \\text{if }i=M\n","\\end{cases}$$\n","\n","$$\\nabla^{+}_{j}u_{i,j} = \\begin{cases}\n","  u_{i,j+1} - u_{i,j} & \\text{if }j<N\\\\ \n","  0& \\text{if }j=N\n","\\end{cases}\n","$$\n"]},{"cell_type":"code","metadata":{"id":"tPPwi30VGle7"},"source":["def im_fwd_gradient(image: np.matrix):\n","    \"\"\"\n","    Discrete gradient of an image using forward differences, with homogeneous Neuman boundary conditions.\n","\n","    :param u: image (MxN)\n","            \n","    :return gradu_j: partial derivative in the j (rows) direction (also x direction)\n","    :return gradu_i: partial derivative in the i (cols) direction (also y direction)\n","    \"\"\"\n","    # TODO: Get the size of the image\n","    image_shape = ()\n","    \n","    # TODO: Calculate both gradients\n","    gradu_i = np.matrix([])\n","    gradu_j = np.matrix([])\n","    return gradu_i, gradu_j"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m = np.array([[1,2,0],[-1,4,3],[3,-5,1]])\n","m"],"metadata":{"id":"oEzTz8mVu3qd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gradu_i,gradu_j=im_fwd_gradient(m)\n","print(f\"forward row gradient: \\n{gradu_i},\\nforward column gradient: \\n{gradu_j}\")"],"metadata":{"id":"eEc2h96Lu4SV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TFd77jw9Gle7"},"source":["#### **1.2 im_bwd_divergence**"]},{"cell_type":"markdown","metadata":{"id":"WS98wPDS1DYy"},"source":["For matrix $u$, with $u^{1} = \\nabla^{+}_{i}u_{i,j}$ and $u^{2} = \\nabla^{+}_{j}u_{i,j}$:\n","\n","$$(div^{-}\\,u) = \n","\\begin{cases}\n","  u^{1}_{i,j} - u^{1}_{i-1,j} & \\text{if }1< i<M\\\\ \n","  u^{1}_{i,j} & \\text{if }i=1\\\\\n","  - u^{1}_{i-1,j} & \\text{if }i=M\n","\\end{cases}\n","+\n","\\begin{cases}\n","  u^{2}_{i,j} - u^{2}_{i,j-1} & \\text{if }1< j<N\\\\ \n","  u^{2}_{i,j} & \\text{if }j=1\\\\\n","  - u^{2}_{i,j-1} & \\text{if }j=N\n","\\end{cases}\n","$$"]},{"cell_type":"code","metadata":{"id":"lHdjZJ_-Gle8"},"source":["def im_bwd_divergence(gradient_i: np.matrix,\n","                      gradient_j: np.matrix):\n","    \"\"\"\n","    Discrete divergence of a vector field using backwards differences. \n","    This is the negative transpose of the im_fwd_gradient\n","    \n","    :param gradient_i: component of g in the direction j (rows) (also x direction)\n","    :param gradient_j: component of g in the direction i (cols) (also y direction)\n","    \n","    :return divg: backwards divergence of g\n","    \"\"\"\n","    divg = np.matrix([])\n","    \n","    # TODO: Backwards i partial derivative of gradient_i\n","    \n","\n","    # TODO: Backwards j partial derivative of gradient_j\n","    \n","    return divg"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["im_bwd_divergence(gradu_i,gradu_j)"],"metadata":{"id":"tef1N-7FvEqc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qd0UNkUlGle8"},"source":["### **2. Complete the functions denoise_energy and denoise_energy_gradient following the comments provided in the code. Avoid building sparse huge matrices.**"]},{"cell_type":"markdown","metadata":{"id":"H_qwqzQjV1GX"},"source":["Let:\n","\n","+ $u \\rightarrow$ Decision variable (final image)\n","+ $f \\rightarrow$ Noisy data\n","\n","Then,\n","\n","\\begin{equation}\n","\tE(u) = \\overbrace{\\sum_{i = 1}^M\\sum_{j = 1}^N c_{ij}|\\nabla^+u_{ij}|^2}^{\\text{regularization}}  +\t\\beta \\overbrace{\\sum_{i = 1}^M\\sum_{j = 1}^N (u_{ij} -\tf_{ij})^2,}^{\\text{data attachment}}\n","\\end{equation}"]},{"cell_type":"code","metadata":{"id":"Z5Pv36snGle8"},"source":["def denoise_energy(image: np.matrix,\n","                   noise: np.matrix,\n","                   coefficients: np.matrix,\n","                   beta: float):\n","    \"\"\"\n","    Evaluates the denoising energy from an image and the noisy\n","    data (see the guide)\n","\n","    :param  image: target image (MxN)\n","    :param  noise: (MxN) noisy data for attachment term \n","    :param  coefficients: (MxN) coefficients image for regularization term\n","    :param  beta: (1x1) weight of attachment term\n","\n","    :return e: energy value\n","    \"\"\"\n","    energy = 0.0\n","    \n","    # TODO: Calculate the regularization term\n","    \n","    # TODO: Calculate the data attachment term\n","\n","    return energy"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_real = np.array([[1,2,3], [5,6,7], [8,9,10]])\n","image_noisy = image_real + 5 * np.ones_like(image_real)\n","denoise_energy(image_real,\n","               image_noisy,\n","               np.ones_like(image_noisy),\n","               .05)\n","# The solution to the above example should be 92.25"],"metadata":{"id":"IRwlgpHBus2t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AdkShepzV9Cg"},"source":["Also,\n","\n","\\begin{equation}\n","\t\\nabla E(u) = -2div^{-}(C\\nabla^+u_{ij}) + 2\\beta(u - f)\n","\\end{equation}"]},{"cell_type":"code","metadata":{"id":"NV45ms7uGle9"},"source":["def denoise_energy_gradient(image: np.matrix,\n","                            noise: np.matrix,\n","                            coefficients: np.matrix,\n","                            beta: float):\n","    \"\"\"\n","    Evaluates the denoising energy gradient from an image and the noisy data (see the guide)\n","\n","    :param image: target image (MxN)\n","    :param noise: (MxN) noisy data for attachment term \n","    :param coefficients: (MxN) coefficients image for regularization term\n","    :param beta: (1x1) weight of attachment term\n","\n","    :return grade : (MxN) gradient of energy at u\n","\n","    Hint: For entry wise multiplication of numpy.matrix objects use np.multiply\n","    \"\"\"\n","    grade = 0.0\n","    \n","    # TODO: Calculate the gradient of regularization term\n","    \n","    # TODO: Calculate the gradient of data attachment term\n","    \n","    # TODO: Calculate the gradient\n","    \n","    return grade"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_real = np.array([[1,2,3], [5,6,7], [8,9,10]])\n","image_noisy = image_real + 5 * np.ones_like(image_real)\n","denoise_energy_gradient(image_real,\n","               image_noisy,\n","               np.ones_like(image_noisy),\n","               .05)"],"metadata":{"id":"WRnZPdnt0QT8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B88lJGSRGle9"},"source":["### **3. Run the blocks with different denoising parameters (&beta; and c). In the report, show results with different values of &beta; and different images c (you can use the examples provided in the notebook). Describe the effect of these parameters in the result.**"]},{"cell_type":"markdown","metadata":{"id":"gNKaD7BLGle-"},"source":["**Load the image**"]},{"cell_type":"code","metadata":{"id":"-G8gLw-KGle-"},"source":["images_dir = os.path.abspath(\"../images\")\n","image_real = np.array(Image.open(os.path.join(images_dir, \"lena.pgm\")))\n","plt.figure()\n","plt.title(\"Real image\")\n","plt.imshow(image_real, cmap = \"gray\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AY0MXxjRGle-"},"source":["**Add noise to the image**"]},{"cell_type":"code","metadata":{"id":"scxpC3UCGle-"},"source":["image_noisy = image_real + np.random.uniform(high = 50, size = image_real.shape)\n","plt.figure()\n","plt.title(\"Noisy data\")\n","plt.imshow(image_noisy, cmap = \"gray\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gJXlcTixGle_"},"source":["**Run the Gradient Descent**"]},{"cell_type":"code","metadata":{"id":"_oq7ub_4Gle_"},"source":["# Define the gradient descent parameters\n","callback_params = {\n","    \"noise\": image_noisy,\n","    \"coefficients\": np.ones_like(image_noisy),    # --> CHANGE THIS AND COMPARE\n","    \"beta\": .05                                   # --> CHANGE THIS AND COMPARE\n","}\n","step_size = .01                                   # --> CHANGE THIS AND COMPARE\n","max_iterations = 200                              # --> CHANGE THIS AND COMPARE\n","tolerance = .1                                    # --> CHANGE THIS AND COMPARE\n","\n","# Run the gradient descent\n","image_gd = gradient_descent(callback = denoise_energy_gradient,\n","                            callback_params = callback_params, \n","                            initial_condition = image_noisy, \n","                            step_size = step_size, \n","                            max_iterations = max_iterations,\n","                            tolerance = tolerance)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7N8ReyIaGle_"},"source":["**Show the results**"]},{"cell_type":"code","metadata":{"id":"c0fnCC2-GlfA"},"source":["# Show the different images\n","fig, (ax1, ax2, ax3) = plt.subplots(nrows = 1, ncols = 3, figsize = (20, 12))\n","ax1.imshow(image_real, cmap = \"gray\")\n","ax1.set_title(\"Real image\")\n","ax2.imshow(image_noisy, cmap = \"gray\")\n","ax2.set_title(\"Noisy data\")\n","ax3.imshow(image_gd, cmap = \"gray\")\n","ax3.set_title(\"Gradient Descent image\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4Jk4yFTGlfA"},"source":["# Show the absolute error among images\n","noise = abs(image_real - image_noisy)\n","denoised = abs(image_gd - image_real)\n","\n","# Plot\n","fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 12))\n","ax1.imshow(noise, cmap = \"gray\")\n","ax1.set_title(\"Absolute error: real image vs noisy\")\n","ax2.imshow(denoised, cmap = \"gray\")\n","ax2.set_title(\"Absolute error: real image vs denoised\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z99GpI5LGlfA"},"source":["plt.figure(figsize = (20, 12))\n","plt.imshow(abs(image_gd - image_noisy), cmap = \"gray\")\n","plt.title('method noise')"],"execution_count":null,"outputs":[]}]}