{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"RnhalJM-lVeX"},"source":["# Group Members:\n","\n","*   Name 1\n","*   Name 2\n","*   Name 3"]},{"cell_type":"markdown","metadata":{"id":"ZDTkeGKWlX6a"},"source":["# Lab 3 Assignment (Part 1)"]},{"cell_type":"code","metadata":{"id":"1KgGyQOKv67h"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","from mpl_toolkits.mplot3d import Axes3D\n","from math import sqrt\n","from IPython.display import clear_output, display"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uyKDnzgbv67m"},"source":["#### **1. Which of the following matrices are positive, semipositive or negative definite?**"]},{"cell_type":"code","metadata":{"id":"hGmEy7ACv67m"},"source":["# Define the matrixes\n","D1 = np.matrix([[-2, 0], [0, 4]])\n","D2 = np.matrix([[0, 0], [0, 2]])\n","D3 = np.matrix([[0.5, 0], [0, 1.5]])\n","A1 = np.matrix([[1, -3], [-3, 1]])\n","A2 = np.matrix([[1, -1], [-1, 1]])\n","A3 = np.matrix([[1, -.5], [-.5, 1]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sDjpmW6Gv67n"},"source":["# TODO: Build up the code to demonstrate which matrices are\n","#       positive definite and reasoning what you are implementing"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E9jNg5i1v67n"},"source":["#### **2. The Python script quad_fun_main plots contours of the quadratic functions g<sub>i</sub>(x) =〈x, D<sub>i</sub>x〉and f<sub>i</sub>(x)  =〈x, A<sub>i</sub>x〉for  the  above  matrices.** \n","\n","- How  many  minima  does  each function have?  \n","\n","Answer: \n","\n","- Which is the relation between the g<sub>i</sub> and f<sub>i</sub>? \n","\n","Answer: "]},{"cell_type":"code","metadata":{"id":"HWS0GibAv67n"},"source":["def quad_fun_main(matrixes: dict):\n","    # Define the x1 and x2 axis and mesh to draw the level lines\n","    x1 = np.arange(start = -10, stop = 10.1, step = 0.1)\n","    x2 = np.arange(start = -10, stop = 10.1, step = 0.1)\n","    X, Y = np.meshgrid(x1, x2)\n","\n","    # Calculate Z\n","    x = np.matrix([X.T.flatten(), Y.T.flatten()])\n","    results = {}\n","    for key, matrix in matrixes.items():\n","        print(\"Procesing {}\".format(key))\n","        aux = np.matmul(matrix, x)\n","        result = np.zeros((x1.shape[0], x2.shape[0]))\n","        for j in range(result.shape[0]):\n","            for i in range(result.shape[1]):\n","                result[i, j] = (x[:, result.shape[0] * j + i].T * aux[:, result.shape[0] * j + i])\n","        if key.startswith(\"D\"):\n","            matrix_name = \"G{}\".format(key[-1])\n","        else:\n","            matrix_name = \"F{}\".format(key[-1])\n","        print(\"\\tSaving as {}\".format(matrix_name))\n","        results[matrix_name] = result\n","\n","    # Plot the contours\n","    fig, axs = plt.subplots(nrows = int(len(results) / 3), \n","                        ncols = 3, \n","                        figsize = (14,8))\n","    for row in range(axs.shape[0]):\n","        for column in range(axs.shape[1]):\n","            matrix_name = list(results.keys())[axs.shape[0] * row + row + column]\n","            Z = results.get(matrix_name)\n","            axs[row, column].contour(X, Y, Z, \n","                                     corner_mask = False, levels = 150, \n","                                     linewidths=(1,), cmap = cm.coolwarm)\n","            axs[row, column].set_title(matrix_name)    \n","        \n","    # Plot the surfaces\n","    fig = plt.figure(figsize=(16, 10))\n","    for row in range(axs.shape[0]):\n","        for column in range(axs.shape[1]):\n","            ax = fig.add_subplot(axs.shape[0], axs.shape[1], axs.shape[0] * row + row + column + 1, projection='3d')\n","            matrix_name = list(results.keys())[axs.shape[0] * row + row + column]\n","            Z = results.get(matrix_name)\n","            ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n","                               linewidth=0, antialiased=False)\n","            ax.set_title(matrix_name)    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XTfpGV1uv67o"},"source":["# Define the matrixes dictionary\n","matrixes = {\"D1\": D1, \"D2\": D2, \"D3\": D3, \"A1\": A1, \"A2\": A2, \"A3\": A3}\n","\n","# Run the quad_fun_main\n","quad_fun_main(matrixes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UnnhjFWgv67p"},"source":["#### **3. Complete the code of the Python function grad_descent().Follow the comments in the code.**"]},{"cell_type":"code","metadata":{"id":"zxfKj-AGv67p"},"source":["def multiply_ax(x):\n","    return A * x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eeC6ZfxCv67p"},"source":["def gradient_descent(callback,\n","                     b: np.matrix,\n","                     callback_params: dict,\n","                     initial_condition: np.matrix,\n","                     max_iters: int,\n","                     tolerance: float,\n","                     fig = None,\n","                     ax = None):\n","    \"\"\"\n","    Implementation of the gradient descent algorithm (a\n","    gradient descent scheme with optimal adaptive step size) for the minimization\n","    of quadratic problems \n","\n","       f(x) = 1/2 x'Ax - bx. \n","\n","    It uses function handles. It requires a handle to a Python function that implements the product of matrix A with x.\n","\n","    :param callback: handle (pointer) to a Python function implementing the product with matrix A.\n","    :param b: vector b, can be in matrix form (MxN)\n","    :param callback_params: a dictionary with the callback function params\n","    :param x0: initial condition, same dimensions as b (MxN)\n","    :param max_iters: maximum number of iterations\n","    :param tolerance: tolerance for the stopping condition (it stop when the norm of the gradient is below the tolerance)\n","\n","    :return x: value found (MxN)\n","    :return fs: evolution of the target function (total_iters x 1 vector)\n","    \"\"\"\n","    Ax = callback(initial_condition, **callback_params)\n","    r = None                        # TODO: compute residual (the gradient)\n","    nr = None                       # TODO: inner product of gradient\n","    \n","    # Note: since the variables can be stored as matrices (for example, x is an image)\n","    #       we use np.multiply(x1, x2) to compute the dot products.\n","\n","    # Allocate memory for vector of energy values of the iterates\n","    fs  = []\n","    current_value = initial_condition\n","\n","    # Start loop \n","    it = 0\n","\n","    while (sqrt(nr) > tolerance) and (it < max_iters):\n","        # Increase iteration counter\n","        it = it + 1\n","        print(\"[{} of {}]\\t-> |grad f(x)| = {}\".format(it, max_iters, nr))\n","\n","        # TODO: Compute quadratic energy f = .5 <Ax - b,x> - 0.5* <b,x>\n","        value = None\n","        fs.append(value)\n","\n","        # 1. line search in r - compute time step alpha\n","        Ar = callback(r, **callback_params)\n","        alpha = nr / (np.multiply(r, Ar).sum())\n","\n","        previous_value = current_value # keep x_old - just for visualization\n","\n","        #2. TODO: update point x\n","        current_value = None\n","\n","        # TODO: compute new residual r = Ax - b\n","        Ax = callback(current_value, **callback_params)\n","        r = None                        # TODO: compute residual (the gradient)\n","        nr = None                       # TODO: inner product of gradient\n","    \n","        # ----- plot current position! Just for visualization purposes -----\n","        if current_value.shape == (2, 1):\n","            if not ax:\n","                fig, ax = plt.subplots()\n","            ax.plot(current_value[0, 0], current_value[1, 0], marker = 'o', color = \"k\")\n","            ax.plot([previous_value[0, 0], current_value[0, 0]], \n","                    [previous_value[1, 0], current_value[1, 0]], \"-k\")\n","            clear_output(wait=True)\n","            display(fig) \n","        elif current_value.shape[0] > 1 and current_value.shape[1] > 1:\n","            # if x is a matrix (an image) (visualization of denoising) \n","            if it % 10 == 0 or it == 1:\n","                if not ax:\n","                    fig, ax = plt.subplots(figsize = (12, 8))\n","                ax.imshow(current_value, cmap = \"gray\")\n","                clear_output(wait=True)\n","                display(fig) \n","    \n","    print(\"[{} of {}]\\t-> |grad f(x)| = {}\".format(it, max_iters, nr))\n","    return current_value, np.matrix(fs).T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h96RXJBbv67r"},"source":["#### **4. Complete the code of the Python function conj_grad. Follow the comments in the code**"]},{"cell_type":"code","metadata":{"id":"ZAR1na7zv67r"},"source":["def conjugate_gradient(callback,\n","                       b: np.matrix,\n","                       callback_params: dict,\n","                       initial_condition: np.matrix,\n","                       tolerance: float,\n","                       max_iters: int,\n","                       fig = None,\n","                       ax = None):\n","    \"\"\"\n","    implementation of the conjugate gradient algorithm for the minimization of quadratic problems \n","\n","       f(x) = 1/2 x'Ax - bx. \n","\n","    It uses function handles. It requires a handle to a Python function that implements the product of matrix A with x.\n","\n","    :param callback: handle (pointer) to a Python function implementing the product with matrix A. \n","    :param callback_params: dictionary containing the params for the callback functions\n","    :param b: vector b, can be in matrix form (MxN)\n","    :param initial_conditions: initial condition, same dimensions as b (MxN)\n","    :param max_iters: maximum number of iterations\n","    :param tolerance: tolerance for the stopping condition (it stop when the norm of the gradient is below the tolerance)\n","\n","    :return x: value found (MxN)\n","    :return fs: evolution of the target function (total_iters x 1 vector)\n","    \"\"\"\n","\n","    Ax = callback(initial_condition, **callback_params)\n","    r = None                        # TODO: compute residual (the gradient)\n","    nr = None                       # TODO: inner product of gradient\n","    d = None                        # TODO: first descent direction is the negative grad.\n","    \n","    # Note: since the variables can be stored as matrices (for example, x is an image)\n","    #       we use np.multiply(x1, x2) to compute the dot products.\n","\n","    # Allocate memory for vector of energy values of the iterates\n","    fs  = []\n","    current_value = initial_condition\n","\n","    # Start loop \n","    it = 0\n","\n","    while (sqrt(nr) > tolerance) and (it < max_iters):\n","        # Increase iteration counter\n","        it = it + 1\n","        if it % 10 == 0:\n","            print(\"[{} of {}]\\t-> |grad f(x)| = {}\".format(it, max_iters, nr))\n","\n","        # Compute quadratic energy f = .5 <Ax - b,x> - 0.5* <b,x>\n","        fs.append(0.5 * np.multiply(r, current_value).sum() - 0.5 * np.multiply(b, current_value).sum())\n","\n","        # 1. TODO: line search in d - compute time step alpha\n","        Ad = callback(d, **callback_params)\n","        alpha = None\n","\n","        previous_value = current_value # keep x_old - just for visualization\n","\n","        #2. TODO: update point x\n","        current_value = None\n","\n","        # compute new residual r = Ax - b\n","        Ax = callback(current_value, **callback_params)\n","        r = None                          # TODO: compute residual (the gradient)\n","        nr_old = nr                       # inner product of the old residual\n","        nr = None                         # TODO: squared norm of gradient\n","\n","        # A-orthogonalization of r (Gram-Schmidt)\n","        # new search direction d is a linear combination of r and previous d\n","        # chosen so that it is A-orthogonal with the previous search directions\n","        beta_2 = None\n","        d = None\n","        \n","        # ----- plot current position! Just for visualization purposes -----\n","        if current_value.shape == (2, 1):\n","            if not ax:\n","                fig, ax = plt.subplots()\n","            ax.plot(current_value[0, 0], current_value[1, 0], marker = 'o', color = \"r\")\n","            ax.plot([previous_value[0, 0], current_value[0, 0]], \n","                    [previous_value[1, 0], current_value[1, 0]], \"-r\")\n","            clear_output(wait=True)\n","            display(fig) \n","        elif current_value.shape[0] > 1 and current_value.shape[1] > 1:\n","            # if x is a matrix (an image) (visualization of denoising) \n","            if it % 10 == 0:\n","                if not ax:\n","                    fig, ax = plt.subplots(figsize = (12, 8))\n","                ax.imshow(current_value, cmap = \"gray\")\n","                clear_output(wait=True)\n","                display(fig) \n","                \n","    print(\"[{} of {}]\\t-> |grad f(x)| = {}\".format(it, max_iters, nr))\n","    return current_value, np.matrix(fs).T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rDkZ-XnOv67s"},"source":["#### **5. Run the functions conj_grad_test_i with i = 1, 2, 3.These script computes the minima of quadratic functions in R2 and R100. Complete the code if needed and answerthe questions in the code.**"]},{"cell_type":"code","metadata":{"id":"DCO9IU3lv67t"},"source":["A = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8r7MA27Wv67t"},"source":["**conj_grad_test_1**"]},{"cell_type":"code","metadata":{"id":"6hTcAj_9v67t"},"source":["def conj_grad_test_1():\n","    global A\n","    # We start by visualizing a problem in IR^2, and comparing the gradient descent with the conjugate gradient\n","\n","    # --------------------------------------------------------------------------------------\n","    # create grid to plot contours of quadratic function -----------------------------------\n","    # --------------------------------------------------------------------------------------\n","    x1 = np.arange(start = -10, stop = 10.1, step = 0.1)\n","    x2 = np.arange(start = -10, stop = 10.1, step = 0.1)\n","    X, Y = np.meshgrid(x1, x2)\n","\n","    # matrix containing all grid points as columns\n","    x = np.matrix([X.T.flatten(), Y.T.flatten()])\n","\n","    # --------------------------------------------------------------------------------------\n","    # start with a trivial example: a quadratic function based on the identity matrix ------\n","    # --------------------------------------------------------------------------------------\n","    A = np.matrix([[1, 0], [0, 1]])\n","    b = np.matrix([[4], [3]])\n","\n","    # evaluate function for all points in x\n","    Ax = A * x\n","    bx = b.T * x\n","\n","    result = np.zeros((x1.shape[0], x2.shape[0]))\n","    for j in range(result.shape[0]):\n","        for i in range(result.shape[1]):\n","            result[i, j] = 0.5 * x[:, result.shape[0] * j + i].T * Ax[:, result.shape[0] * j + i] - bx[:, result.shape[0] * j + i]\n","\n","    # Plot the contour of f\n","    fig = plt.figure()\n","    plt.contour(X, Y, result, \n","                corner_mask = False, levels = 150, \n","                linewidths=(1,), cmap = cm.coolwarm)\n","    plt.title('Level lines of f(x) = 1/2 x^T I x - bx')\n","\n","    # run gradient descent \n","    tolerance = 1e-5\n","    max_iters = 1000\n","    x1 = gradient_descent(callback = multiply_ax,\n","                          b = b, \n","                          callback_params = {},  \n","                          initial_condition = np.matrix([[-7.5], [-7.5]]),  # TRY several starting points\n","                          tolerance = tolerance,\n","                          max_iters = max_iters, \n","                          fig = fig,\n","                          ax = plt.gca())\n","\n","    # How many iterations does the gradient descent need to converge? Why?\n","    # How many iterations would the conjugate gradient need to converge? Why?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ULNZhsamv67t"},"source":["conj_grad_test_1()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4fwrlTOJv67u"},"source":["**conj_grad_test_2**"]},{"cell_type":"code","metadata":{"id":"JlRjoZLAv67u"},"source":["def conj_grad_test_2():\n","    global A\n","    # We start by visualizing a problem in IR^2, and comparing the gradient descent with the conjugate gradient\n","\n","    # --------------------------------------------------------------------------------------\n","    # create grid to plot contours of quadratic function -----------------------------------\n","    # --------------------------------------------------------------------------------------\n","    x1 = np.arange(start = -10, stop = 10.1, step = 0.1)\n","    x2 = np.arange(start = -10, stop = 10.1, step = 0.1)\n","    X, Y = np.meshgrid(x1, x2)\n","\n","    # matrix containing all grid points as columns\n","    x = np.matrix([X.T.flatten(), Y.T.flatten()])\n","\n","    # --------------------------------------------------------------------------------------\n","    # a more general quadratic function ----------------------------------------------------\n","    # --------------------------------------------------------------------------------------\n","    A = np.matrix([[1, .9], [.9, 1]])\n","    b = np.matrix([[1], [2]])\n","\n","    # evaluate function for all points in x\n","    Ax = A * x\n","    bx = b.T * x\n","\n","    result = np.zeros((x1.shape[0], x2.shape[0]))\n","    for j in range(result.shape[0]):\n","        for i in range(result.shape[1]):\n","            result[i, j] = 0.5 * x[:, result.shape[0] * j + i].T * Ax[:, result.shape[0] * j + i] - bx[:, result.shape[0] * j + i]\n","\n","    # Plot the contour of f\n","    fig = plt.figure()\n","    ax = plt.gca()\n","    plt.contour(X, Y, result, \n","                corner_mask = False, levels = 150, \n","                linewidths=(1,), cmap = cm.coolwarm)\n","    plt.title('Level lines of f(x) = 1/2 x^T I x - bx')\n","\n","    # run gradient descent \n","    tolerance = 0.00001\n","    max_iters = 1000\n","    x1 = gradient_descent(callback = multiply_ax,\n","                          b = b, \n","                          callback_params = {},  \n","                          initial_condition = np.matrix([[5], [-5]]),  # TRY several starting points\n","                          tolerance = tolerance,\n","                          max_iters = max_iters, \n","                          fig = fig,\n","                          ax = ax)\n","\n","    # run gradient descent \n","    tolerance = 0.00001\n","    max_iters = 100\n","    x1 = conjugate_gradient(callback = multiply_ax,\n","                            b = b, \n","                            callback_params = {},  \n","                            initial_condition = np.matrix([[5], [-5]]),  # TRY several starting points\n","                            tolerance = tolerance,\n","                            max_iters = max_iters, \n","                            fig = fig,\n","                            ax = ax)\n","\n","    # How many iterations does the gradient descent need to converge?\n","\n","    # Does the performance of the gradient descent depend on the position of the initial condition? \n","\n","    # Are there any initial conditions for which the gradient descent converges in one iteration?\n","\n","    # Which is the angle between consecutive descent directions for the gradient descent? Why?\n","\n","    # How many iterations would the conjugate gradient need to converge? Why?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mj9hqggOv67u"},"source":["conj_grad_test_2()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w6O5kSC6v67u"},"source":["def conj_grad_test_3():\n","    global A\n","    # Now we compare the gradient descent and the conjugate gradient for a problem in IR^100. \n","\n","    # Define the problem matrix. Is it symmetric? Is it positive definite?\n","    A = np.matrix(np.ones(shape = (100, 100)) + np.diag(np.arange(0, 100)))\n","    b = np.matrix(np.ones(shape = (100, 1)))\n","\n","    # run gradient descent \n","    tolerance = 10 ** -5\n","    max_iters = 10 ** 6\n","    x1, fs_1 = gradient_descent(callback = multiply_ax,\n","                          b = b, \n","                          callback_params = {},  \n","                          initial_condition = np.zeros_like(b),  # TRY several starting points\n","                          tolerance = tolerance,\n","                          max_iters = max_iters)\n","\n","    # run gradient descent \n","    tolerance = 10 ** -5\n","    max_iters = 100\n","    x2, fs_2 = conjugate_gradient(callback = multiply_ax,\n","                            b = b, \n","                            callback_params = {},  \n","                            initial_condition = np.zeros_like(b),  # TRY several starting points\n","                            tolerance = tolerance,\n","                            max_iters = max_iters)\n","\n","    m = -0.5 * b.T * np.linalg.lstsq(A, b)[0]\n","    m = m[0,0]\n","\n","    fig, ax = plt.subplots(figsize = (14, 8))\n","    ax.semilogy(fs_1 - m)\n","    ax.semilogy(fs_2 - m, 'r') \n","    ax.legend(['gradient descent', 'conjugate gradient'])\n","    ax.set_title('logarithmic plot of f(x_i) - f^*')\n","\n","    # Which one converges faster?\n","\n","    # Which is the order of convergence of the gradient descent?\n","\n","    # Why is the logarithmic plot of the error for the gradient descent linear?\n","\n","    # Which is the maximum number of iterations needed for the conjugate gradient?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rs1x14bGv67v"},"source":["conj_grad_test_3()"],"execution_count":null,"outputs":[]}]}